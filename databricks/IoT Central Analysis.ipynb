{"cells":[{"cell_type":"markdown","source":["# IoT Central streaming to Azure Databricks\n\nThis python script is an example of how [IoT Central](https://azure.microsoft.com/services/iot-central/) can stream data to [Azure Databricks](https://azure.microsoft.com/services/databricks/) using Apache Spark. \n\nWhen this notebook runs in a Databricks workspace, the Python script:\n\n1. Reads the streaming measurement data from from an IoT Central application.\n1. Plots averaged humidity data by device to show a smoother plot.\n1. Stores the data in the cluster.\n1. Displays box plots with any outliers from the stored data.\n\n## Configuring event hub connection strings\n\nIoT Central can be set up to export data to Azure Event Hubs using the **Continuous data export** feature. This example uses a single event hub for streaming telemetry. \n\nThe connection string in the following cell is for the telemetry event hub. For more information, see the how-to guide [Extend Azure IoT Central with custom analytics](https://docs.microsoft.com/azure/iot-central/howto-create-custom-analytics)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n###### Event Hub Connection string ######\ntelementryEventHubConfig = {\n  'eventhubs.connectionString' : '{your Event Hubs connection string}'\n}"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["##Helper functions\nThese helper functions manipulate the Spark [DataFrames](https://docs.azuredatabricks.net/spark/latest/dataframes-datasets/index.html).\n\n#### Removing quotations\nSome data comes through from the event hub with quotation marks, you want to clean this."],"metadata":{}},{"cell_type":"code","source":["@udf\ndef removeQuotations(value):\n  return value.replace('\"', '')"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Getting nested items from a dictionary\nYou need to get items from deep in a dictionary that may not exist, this function checks if items exist, and if they do, retrieves them."],"metadata":{}},{"cell_type":"code","source":["#Gets from a multi-level dictionary \ndef nestedGet(dictionary, nestedKeys):\n    for key in nestedKeys:\n        dictionary = dictionary.get(key, None)\n        if dictionary is None:\n            return None\n    return dictionary"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Body property extractor\nThis function creates a [User-defined function (UDF)](https://docs.azuredatabricks.net/spark/latest/spark-sql/udf-python.html) that extracts an item nested in the JSON body of an event hub message.\n\nFor example, `bodyPropertyExtractorBuilder(['properties', 'location'])` returns a UDF that extracts `body.properties.location` if it exists."],"metadata":{}},{"cell_type":"code","source":["import json\n\ndef bodyPropertyExtractorBuilder(nestedKeys):\n  def bodyPropertyExtractor(body):\n    decodedBody =  json.loads(body.decode(\"utf-8\"));\n    return nestedGet(decodedBody, nestedKeys)\n  return bodyPropertyExtractor"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Telemetry query\n#### Initial query\n\nThis creates a streaming DataFrame from the telemtry event hub. A streaming DataFrame continuously updates as more data arrives."],"metadata":{}},{"cell_type":"code","source":["telemetryDF = spark \\\n  .readStream \\\n  .format(\"eventhubs\") \\\n  .options(**telementryEventHubConfig) \\\n  .load()\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Extract the required data\nThis creates a new streaming DataFrame that contains the:\n- Device Id from the event hub message's system properties\n- Enqueued time from the event hub message's system properties\n- humidity from the event hub message's body\n\nThe code uses the `removeQuotations` and `bodyPropertyExtractorBuilder` functions defined previously."],"metadata":{}},{"cell_type":"code","source":["humidityUdf = udf(bodyPropertyExtractorBuilder(['humidity']), FloatType())\ntelemetryDF = telemetryDF.select(\n    removeQuotations(telemetryDF.systemProperties['iothub-connection-device-id']).alias('deviceId'),\n    removeQuotations(telemetryDF.systemProperties['iothub-enqueuedtime']).cast(\"timestamp\").alias('enqueuedtime'),\n    humidityUdf('body').alias('humidity')\n  )"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Plot the telemetry\n\nThe following code uses a window to calculate rolling averages by device Id.\n\nBecause the example is still using a streaming DataFrame, the chart updates continuously."],"metadata":{}},{"cell_type":"code","source":["smoothTelemetryDF = telemetryDF.groupBy(\n  window('enqueuedtime', \"10 minutes\", \"5 minutes\"),\n  'deviceId'\n).agg({'humidity': 'avg'})\ndisplay(smoothTelemetryDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Analyze the telemetry further \n\nTo perform more complex analysis, the following cell continuously writes the streaming data to a table in the cluster. The amount of data stored will continue to grow, so in a production system you should periodically delete or archive old telemetry data.\n\n###Write streaming data query results to a database\nWrites the final telemetryDF DataFrame to a [database table](https://docs.azuredatabricks.net/user-guide/tables.html) in the cluster. You could choose to write the telemetry to another storage location such as an external database or blob store.\n\nFor more information, see [Streaming Data Sources and Sinks](https://docs.azuredatabricks.net/spark/latest/structured-streaming/data-sources.html)."],"metadata":{}},{"cell_type":"code","source":["telemetryDF \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/delta/events/_checkpoints/etl-from-json\") \\\n    .table(\"telemetry\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Wait until some streaming data has been written to storage."],"metadata":{}},{"cell_type":"code","source":["from time import sleep\nsleep(60) # wait until some telemtry has been written to storage"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Generate box plots\nThe format of the stored data is not suitable for using the Matplotlib [boxplot](https://matplotlib.org/gallery/statistics/boxplot_demo.html) function. It's also not possible to *pivot* streaming data - this is why the previous cell wrote the streaming data to the filesystem.\n\nThe following code:\n1. Generates a list of device Ids to use as column headings.\n1. Loads and pivots the stored data and then converts it to a pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n1. Uses Matplotlib to generate a [box plot](https://en.wikipedia.org/wiki/Box_plot)\n\nA box plot is a way to show the spread of data and any outliers. The chart shows hourly box plots for each device. You need to wait for some time to see multiple hourly plots.\n\nNote: this chart isn't based on streaming data so you need to manually update it by re-running the cell."],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\n# Get list of distinct deviceId values\ndevicelist = spark.table('telemetry').select(collect_set('deviceId').alias('deviceId')).first()['deviceId']\n\n# Pivot and convert to a pandas dataframe\npdDF = spark.table('telemetry').groupBy('enqueuedtime').pivot('deviceId').mean('humidity').orderBy('enqueuedtime').withColumn('hour', date_trunc('hour', 'enqueuedtime')).toPandas()\n\n# Use the pandas plotting function\nplt.clf()\npdDF.boxplot(column=devicelist, by=['hour'], rot=90, fontsize='medium', layout=(2,2), figsize=(20,8))\ndisplay()"],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"PythonTest","notebookId":664188933124706},"nbformat":4,"nbformat_minor":0}
