{"cells":[{"cell_type":"markdown","source":["# IoT Central streaming to Azure Databricks\n\nThis python script is an example of how [IoT Central](https://azure.microsoft.com/services/iot-central/) can stream data to [Azure Databricks](https://azure.microsoft.com/services/databricks/) using Apache Spark. \n\nWhen this notebook runs in a Databricks workspace, the Python script:\n\n1. Reads the streaming measurement data from from an IoT Central application.\n1. Plots averaged humidity data by device to show a smoother plot.\n1. Stores the data in the cluster.\n1. Displays box plots with any outliers from the stored data.\n\n## Configuring event hub connection strings\n\nIoT Central can be set up to export data to Azure Event Hubs using the **Continuous data export** feature. This example uses a single event hub for streaming telemetry. \n\nThe connection string in the following cell is for the telemetry event hub. For more information, see the how-to guide [Extend Azure IoT Central with custom analytics](https://docs.microsoft.com/azure/iot-central/howto-create-custom-analytics)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5de9d1fe-edce-45be-a008-134db1998d20"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n###### Event Hub Connection string ######\ntelementryEventHubConfig = {\n  'eventhubs.connectionString' : '{your Event Hubs connection string}'\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"193b5f94-7f96-4ee1-90ad-cf797be3930c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Telemetry query\n#### Initial query\n\nThis creates a streaming DataFrame from the telemtry event hub. A streaming DataFrame continuously updates as more data arrives."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90816d90-5405-4e59-af40-f096cc0a6b5f"}}},{"cell_type":"code","source":["telemetryDF = spark \\\n  .readStream \\\n  .format(\"eventhubs\") \\\n  .options(**telementryEventHubConfig) \\\n  .load()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e8d9664-a42e-4610-970e-4fe15eb26902"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Extract the required data\nThis creates a new streaming DataFrame that contains the:\n- `deviceId` from the event hub message body\n- `enqueuedTime` from the event hub message body\n- `humidity` from the event hub message body\n\nThe code converts the binary body field to JSON and then extracts the required fields from the JSON.\n\nThe `sourceSchema` structure is a partial schema for the `body` field that defines just the fields you need."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ad2acf5-9908-4d39-be4e-e663737a5aab"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e5b20d6-e0bb-42db-9deb-ea6799f1a037"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a schema that describes the Body field\nsourceSchema = StructType([\n  StructField(\"deviceId\", StringType(), True),\n  StructField(\"enqueuedTime\", TimestampType(), True),\n  StructField(\"telemetry\", StructType([\n    StructField('humidity', FloatType(), True),\n    StructField('temperature', FloatType(), True),\n    StructField('pressure', FloatType(), True)\n  ])),\n])\n\n# Convert the binary Body column to a string\ntelemetryDF = telemetryDF.withColumn(\"body\", col(\"Body\").cast(\"string\")).select(col('body'))\n\n# Convert the string to JSON and select the fields you need.\njsonOptions = {\"dateFormat\" : \"yyyy-MM-dd HH:mm:ss.SSS\"}\ntelemetryDF = telemetryDF.withColumn(\"Body\", from_json(telemetryDF.body, sourceSchema, jsonOptions)) \\\n  .select(col('body.deviceId'),col('body.enqueuedTime'),col('body.telemetry.humidity'), \\\n  col('body.telemetry.temperature'),col('body.telemetry.pressure'))\n\n# display(telemetryDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd2dcada-1015-44f7-93e3-68ca4198a0d8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Plot the telemetry\n\nThe following code uses a window to calculate rolling averages by device Id.\n\nBecause the example is still using a streaming DataFrame, the chart updates continuously."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd307580-353f-4ecb-b4a9-4ac31ba88a63"}}},{"cell_type":"code","source":["smoothTelemetryDF = telemetryDF.groupBy(\n  window('enqueuedtime', \"10 minutes\", \"5 minutes\"),\n  'deviceId'\n).agg({'humidity': 'avg'})\ndisplay(smoothTelemetryDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a7f7f84-4833-4b0e-b3cc-367c397a7c59"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Analyze the telemetry further \n\nTo perform more complex analysis, the following cell continuously writes the streaming data to a table in the cluster. The amount of data stored will continue to grow, so in a production system you should periodically delete or archive old telemetry data.\n\n###Write streaming data query results to a database\nWrites the final telemetryDF DataFrame to a [database table](https://docs.azuredatabricks.net/user-guide/tables.html) in the cluster. You could choose to write the telemetry to another storage location such as an external database or blob store.\n\nFor more information, see [Streaming Data Sources and Sinks](https://docs.azuredatabricks.net/spark/latest/structured-streaming/data-sources.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16666d3a-33b0-48e5-b0e8-e2a42f9ebc80"}}},{"cell_type":"code","source":["telemetryDF \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/delta/events/_checkpoints/etl-from-json\") \\\n    .table(\"telemetry\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7265f0c-af8b-46d7-9aa6-6199db6e6ce9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until some streaming data has been written to storage."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55aa2e41-3947-4061-8a15-632a2d39d67a"}}},{"cell_type":"code","source":["from time import sleep\nsleep(60) # wait until some telemtry has been written to storage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac924388-fb48-4fb2-bcf3-16abeecc8b33"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Generate box plots\nThe format of the stored data is not suitable for using the Matplotlib [boxplot](https://matplotlib.org/gallery/statistics/boxplot_demo.html) function. It's also not possible to *pivot* streaming data - this is why the previous cell wrote the streaming data to the filesystem.\n\nThe following code:\n1. Generates a list of device Ids to use as column headings.\n1. Loads and pivots the stored data and then converts it to a pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n1. Uses Matplotlib to generate a [box plot](https://en.wikipedia.org/wiki/Box_plot)\n\nA box plot is a way to show the spread of data and any outliers. The chart shows hourly box plots for each device. You need to wait for some time to see multiple hourly plots.\n\nNote: this chart isn't based on streaming data so you need to manually update it by re-running the cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e32da9e-0173-4d40-b128-cdb7efa8db8e"}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\n# Get list of distinct deviceId values\ndevicelist = spark.table('telemetry').select(collect_set('deviceId').alias('deviceId')).first()['deviceId']\n\n# Pivot and convert to a pandas dataframe\npdDF = spark.table('telemetry').groupBy('enqueuedtime').pivot('deviceId').mean('humidity').orderBy('enqueuedtime').withColumn('hour', date_trunc('hour', 'enqueuedtime')).toPandas()\n\n# Use the pandas plotting function\nplt.clf()\npdDF.boxplot(column=devicelist, by=['hour'], rot=90, fontsize='medium', layout=(2,2), figsize=(20,8))\ndisplay()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ac04140-95ec-497b-84a7-83f8484941e3"}},"outputs":[],"execution_count":0}],"metadata":{"name":"PythonTest","notebookId":1493892649172675,"application/vnd.databricks.v1+notebook":{"notebookName":"IoT Central Analysis","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2829769934171009}},"nbformat":4,"nbformat_minor":0}
